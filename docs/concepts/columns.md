# Columns

Columns are the fundamental building blocks in Data Designer. Each column represents a field in your dataset and defines how to generate itâ€”whether that's sampling from a distribution, calling an LLM, or applying a transformation.

!!! note "The Declarative Approach"
    Columns are **declarative specifications**. You describe *what* you want, and the framework handles *how* to generate itâ€”managing execution order, batching, parallelization, and resources automatically.

## Column Types

Data Designer provides nine built-in column types, each optimized for different generation scenarios.

### ðŸŽ² Sampler Columns

Sampler columns generate data using numerical samplingâ€”fast, deterministic, and ideal for numerical and categorical dataset fields. They're significantly faster than LLMs and can produce data following specific distributions (Poisson for event counts, Gaussian for measurements, etc.).

Available sampler types:

- **UUID**: Unique identifiers
- **Category**: Categorical values with optional probability weights
- **Subcategory**: Hierarchical categorical data (states within countries, models within brands)
- **Uniform**: Evenly distributed numbers (integers or floats)
- **Gaussian**: Normally distributed values with configurable mean and standard deviation
- **Bernoulli**: Binary outcomes with specified success probability
- **Bernoulli Mixture**: Binary outcomes from multiple probability components
- **Binomial**: Count of successes in repeated trials
- **Poisson**: Count data and event frequencies
- **Scipy**: Access to the full scipy.stats distribution library
- **Person**: Realistic synthetic individuals with names, demographics, and attributes
- **Datetime**: Timestamps within specified ranges
- **Timedelta**: Time duration values

!!! tip "Conditional Sampling"
    Samplers support **conditional parameters** that change behavior based on other columns. Want age distributions that vary by country? Income ranges that depend on occupation? Just define conditions on existing column values.

### ðŸ“ LLM-Text Columns

LLM-Text columns generate natural language text: product descriptions, customer reviews, narrative summaries, email threads, or anything requiring semantic understanding and creativity.

Use **Jinja2 templating** in prompts to reference other columns. Data Designer automatically manages dependencies and injects the referenced column values into the prompt.

!!! note "Generation Traces"
    LLM columns can optionally capture a full message trace in a separate `{column_name}__trace` column. Enable traces per-column via `with_trace=True` on the column config, or globally for all columns via `RunConfig(debug_override_save_all_column_traces=True)`. The trace includes the ordered message history for the final generation attempt (system/user/assistant/tool calls/tool results), and may include model reasoning fields when the provider exposes them.

!!! tip "Tool Use in LLM Columns"
    LLM columns can invoke external tools during generation via MCP (Model Context Protocol). Enable tools by setting `tool_alias` to reference a configured `ToolConfig`:

    ```python
    dd.LLMTextColumnConfig(
        name="answer",
        model_alias="nvidia-text",
        prompt="Search for information and answer: {{ question }}",
        tool_alias="search-tools",  # References a ToolConfig
        with_trace=True,  # Capture tool call history
    )
    ```

    When `tool_alias` is set, the model can request tool calls during generation. Data Designer executes the tools via configured MCP providers and feeds results back until the model produces a final answer. See [Tool Use & MCP](tool_use_and_mcp.md) for full configuration details.

### ðŸ’» LLM-Code Columns

LLM-Code columns generate code in specific programming languages. They handle the prompting and parsing necessary to extract clean code from the LLM's responseâ€”automatically detecting and extracting code from markdown blocks. You provide the prompt and choose the model; the column handles the extraction.

Supported languages: **Bash, C, C++, C#, COBOL, Go, Java, JavaScript, Kotlin, Python, Ruby, Rust, Scala, Swift, TypeScript**, plus **SQL** dialects (SQLite, PostgreSQL, MySQL, T-SQL, BigQuery, ANSI SQL).

### ðŸ—‚ï¸ LLM-Structured Columns

LLM-Structured columns generate JSON with a *guaranteed schema*. Define your structure using a Pydantic model or JSON schema, and Data Designer ensures the LLM output conformsâ€”no parsing errors, no schema drift.

Use for complex nested structures: API responses, configuration files, database records with multiple related fields, or any structured data where type safety matters. Schemas can be arbitrarily complex with nested objects, arrays, enums, and validation constraints, but success depends on the model's capabilities.

!!! tip "Schema Complexity and Model Choice"
    Flat schemas with simple fields are easier and more robustly produced across models. Deeply nested schemas with complex validation constraints are more sensitive to model choiceâ€”stronger models handle complexity better. If you're experiencing schema conformance issues, try simplifying the schema or switching to a more capable model.

### âš–ï¸ LLM-Judge Columns

LLM-Judge columns score generated content across multiple quality dimensions using LLMs as evaluators.

Define scoring rubrics (relevance, accuracy, fluency, helpfulness) and the judge model evaluates each record. Score rubrics specify criteria and scoring options (1-5 scales, categorical grades, etc.), producing quantified quality metrics for every data point.

Use judge columns for data quality filtering (e.g., keep only 4+ rated responses), A/B testing generation strategies, and quality monitoring over time.

### ðŸ§¬ Embedding Columns

Embedding columns generate vector embeddings (numerical representations) for text content using embedding models. These embeddings capture semantic meaning, enabling similarity search, clustering, and semantic analysis.

Specify a `target_column` containing text, and Data Designer generates embeddings for that content. The target column can contain either a single text string or a list of text strings in stringified JSON format. In the latter case, embeddings are generated for each text string in the list.

Common use cases:

- **Semantic search**: Generate embeddings for documents, then find similar content by vector similarity
- **Clustering**: Group similar texts based on embedding proximity
- **Recommendation systems**: Match content by semantic similarity
- **Anomaly detection**: Identify outliers in embedding space

!!! note "Embedding Models"
    Embedding columns require an embedding model configured with `EmbeddingInferenceParams`. These models differ from chat completion modelsâ€”they output vectors rather than text. The generation type is automatically determined by the inference parameters type.

### ðŸ§© Expression Columns

Expression columns handle simple transformations using **Jinja2 templates**â€”concatenate first and last names, calculate numerical totals, format date strings. No LLM overhead needed.

Template capabilities:

- **Variable substitution**: Pull values from any existing column
- **String filters**: Uppercase, lowercase, strip whitespace, replace patterns
- **Conditional logic**: if/elif/else support
- **Arithmetic**: Add, subtract, multiply, divide

### ðŸ” Validation Columns

Validation columns check generated content against rules and return structured pass/fail results.

Built-in validation types:

**Code validation** runs Python or SQL code through a linter to validate the code.

**Local callable validation** accepts a Python function directly when using Data Designer as a library.

**Remote validation** sends data to HTTP endpoints for validation-as-a-service. Useful for linters, security scanners, or proprietary systems.

### ðŸŒ± Seed Dataset Columns

Seed dataset columns bootstrap generation from existing data. Provide a real dataset, and those columns become available as context for generating new synthetic data.

Typical pattern: use seed data for one part of your schema (real product names and categories), then generate synthetic fields around it (customer reviews, purchase histories, ratings). The seed data provides realism and constraints; generated columns add volume and variation.

## Shared Column Properties

Every column configuration inherits from `SingleColumnConfig` with these standard properties:

### `name`

The column's identifierâ€”unique within your configuration, used in Jinja2 references, and becomes the column name in the output DataFrame. Choose descriptive names: `user_review` > `col_17`.

### `drop`

Boolean flag (default: `False`) controlling whether the column appears in final output. Setting `drop=True` generates the column (available as a dependency) but excludes it from final output.

**When to drop columns:**

- Intermediate calculations that feed expressions but aren't meaningful standalone
- Context columns used only for LLM prompt templates
- Validation results during development unwanted in production

Dropped columns participate fully in generation and the dependency graphâ€”just filtered out at the end.

### `column_type`

Literal string identifying the column type: `"sampler"`, `"llm-text"`, `"expression"`, etc. Set automatically by each configuration class and serves as Pydantic's discriminator for deserialization.

You rarely set this manuallyâ€”instantiating `LLMTextColumnConfig` automatically sets `column_type="llm-text"`. Serialization is reversible: save to YAML, load later, and Pydantic reconstructs the exact objects.

### `required_columns`

Computed property listing columns that must be generated before this one. The framework derives this automatically:

- For LLM/Expression columns: extracted from Jinja2 template `{{ variables }}`
- For Validation columns: explicitly listed target columns
- For Sampler columns with conditional parameters: columns referenced in conditions

You read this property for introspection but never set itâ€”always computed from configuration details.

### `side_effect_columns`

Computed property listing columns created implicitly alongside the primary column. Currently, only LLM columns produce side effects (trace columns like `{name}__trace` when `with_trace=True` is set on the column or `debug_override_save_all_column_traces` is enabled globally).

For detailed information on each column type, refer to the [column configuration code reference](../code_reference/column_configs.md).
