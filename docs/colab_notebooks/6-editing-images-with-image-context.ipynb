{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40e16af1",
   "metadata": {},
   "source": [
    "# üé® Data Designer Tutorial: Image-to-Image Editing\n",
    "\n",
    "#### üìö What you'll learn\n",
    "\n",
    "This notebook shows how to chain image generation columns: first generate animal portraits from text, then edit those generated images by adding accessories and changing styles‚Äîall without loading external datasets.\n",
    "\n",
    "- üñºÔ∏è **Text-to-image generation**: Generate images from text prompts\n",
    "- üîó **Chaining image columns**: Use `ImageContext` to pass generated images to a follow-up editing column\n",
    "- üé≤ **Sampler-driven diversity**: Combine sampled accessories and settings for varied edits\n",
    "\n",
    "This tutorial uses an **autoregressive** model (one that supports both text-to-image *and* image-to-image generation via the chat completions API). Diffusion models (DALL¬∑E, Stable Diffusion, etc.) do not support image context‚Äîsee [Tutorial 5](https://nvidia-nemo.github.io/DataDesigner/latest/notebooks/5-generating-images/) for text-to-image generation with diffusion models.\n",
    "\n",
    "> **Prerequisites**: This tutorial uses [OpenRouter](https://openrouter.ai) with the Flux 2 Pro model. Set `OPENROUTER_API_KEY` in your environment before running.\n",
    "\n",
    "If this is your first time using Data Designer, we recommend starting with the [first notebook](https://nvidia-nemo.github.io/DataDesigner/latest/notebooks/1-the-basics/) in this tutorial series.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846d9aad",
   "metadata": {},
   "source": [
    "### üì¶ Import Data Designer\n",
    "\n",
    "- `data_designer.config` provides the configuration API.\n",
    "- `DataDesigner` is the main interface for generation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738842c8",
   "metadata": {},
   "source": [
    "### ‚ö° Colab Setup\n",
    "\n",
    "Run the cells below to install the dependencies and set up the API key. If you don't have an API key, you can generate one from [build.nvidia.com](https://build.nvidia.com).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f03293",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -U data-designer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65be55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "from google.colab import userdata\n",
    "\n",
    "try:\n",
    "    os.environ[\"NVIDIA_API_KEY\"] = userdata.get(\"NVIDIA_API_KEY\")\n",
    "except userdata.SecretNotFoundError:\n",
    "    os.environ[\"NVIDIA_API_KEY\"] = getpass.getpass(\"Enter your NVIDIA API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8970dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from pathlib import Path\n",
    "\n",
    "from IPython.display import Image as IPImage\n",
    "from IPython.display import display\n",
    "\n",
    "import data_designer.config as dd\n",
    "from data_designer.interface import DataDesigner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9c7aec",
   "metadata": {},
   "source": [
    "### ‚öôÔ∏è Initialize the Data Designer interface\n",
    "\n",
    "We initialize Data Designer without arguments here‚Äîthe image model is configured explicitly in the next cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6f7185",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_designer = DataDesigner()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c18e7a",
   "metadata": {},
   "source": [
    "### üéõÔ∏è Define an image model\n",
    "\n",
    "We need an **autoregressive** model that supports both text-to-image and image-to-image generation via the chat completions API. This lets us generate images from text and then pass those images as context for editing.\n",
    "\n",
    "- Use `ImageInferenceParams` so Data Designer treats this model as an image generator.\n",
    "- Image-specific options are model-dependent; pass them via `extra_body`.\n",
    "\n",
    "> **Note**: This tutorial uses the Flux 2 Pro model via [OpenRouter](https://openrouter.ai). Set `OPENROUTER_API_KEY` in your environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c88de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PROVIDER = \"openrouter\"\n",
    "MODEL_ID = \"black-forest-labs/flux.2-pro\"\n",
    "MODEL_ALIAS = \"image-model\"\n",
    "\n",
    "model_configs = [\n",
    "    dd.ModelConfig(\n",
    "        alias=MODEL_ALIAS,\n",
    "        model=MODEL_ID,\n",
    "        provider=MODEL_PROVIDER,\n",
    "        inference_parameters=dd.ImageInferenceParams(\n",
    "            extra_body={\"height\": 512, \"width\": 512},\n",
    "        ),\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4883011",
   "metadata": {},
   "source": [
    "### üèóÔ∏è Build the configuration\n",
    "\n",
    "We chain two image generation columns:\n",
    "\n",
    "1. **Sampler columns** ‚Äî randomly sample animal types, accessories, settings, and art styles\n",
    "2. **First image column** ‚Äî generate an animal portrait from a text prompt\n",
    "3. **Second image column with context** ‚Äî edit the generated portrait using `ImageContext`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b54bf99",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_builder = dd.DataDesignerConfigBuilder(model_configs=model_configs)\n",
    "\n",
    "# 1. Sampler columns for diversity\n",
    "config_builder.add_column(\n",
    "    dd.SamplerColumnConfig(\n",
    "        name=\"animal\",\n",
    "        sampler_type=dd.SamplerType.CATEGORY,\n",
    "        params=dd.CategorySamplerParams(\n",
    "            values=[\"cat\", \"dog\", \"fox\", \"owl\", \"rabbit\", \"panda\"],\n",
    "        ),\n",
    "    )\n",
    ")\n",
    "\n",
    "config_builder.add_column(\n",
    "    dd.SamplerColumnConfig(\n",
    "        name=\"accessory\",\n",
    "        sampler_type=dd.SamplerType.CATEGORY,\n",
    "        params=dd.CategorySamplerParams(\n",
    "            values=[\n",
    "                \"a tiny top hat\",\n",
    "                \"oversized sunglasses\",\n",
    "                \"a red bow tie\",\n",
    "                \"a knitted beanie\",\n",
    "                \"a flower crown\",\n",
    "                \"a monocle and mustache\",\n",
    "                \"a pirate hat and eye patch\",\n",
    "                \"a chef hat\",\n",
    "            ],\n",
    "        ),\n",
    "    )\n",
    ")\n",
    "\n",
    "config_builder.add_column(\n",
    "    dd.SamplerColumnConfig(\n",
    "        name=\"setting\",\n",
    "        sampler_type=dd.SamplerType.CATEGORY,\n",
    "        params=dd.CategorySamplerParams(\n",
    "            values=[\n",
    "                \"a cozy living room\",\n",
    "                \"a sunny park\",\n",
    "                \"a photo studio with soft lighting\",\n",
    "                \"a red carpet event\",\n",
    "                \"a holiday card backdrop with snowflakes\",\n",
    "                \"a tropical beach at sunset\",\n",
    "            ],\n",
    "        ),\n",
    "    )\n",
    ")\n",
    "\n",
    "config_builder.add_column(\n",
    "    dd.SamplerColumnConfig(\n",
    "        name=\"art_style\",\n",
    "        sampler_type=dd.SamplerType.CATEGORY,\n",
    "        params=dd.CategorySamplerParams(\n",
    "            values=[\n",
    "                \"a photorealistic style\",\n",
    "                \"a Disney Pixar 3D render\",\n",
    "                \"a watercolor painting\",\n",
    "                \"a pop art poster\",\n",
    "            ],\n",
    "        ),\n",
    "    )\n",
    ")\n",
    "\n",
    "# 2. Generate animal portrait from text\n",
    "config_builder.add_column(\n",
    "    dd.ImageColumnConfig(\n",
    "        name=\"animal_portrait\",\n",
    "        prompt=\"A close-up portrait photograph of a {{ animal }} looking at the camera, studio lighting, high quality.\",\n",
    "        model_alias=MODEL_ALIAS,\n",
    "    )\n",
    ")\n",
    "\n",
    "# 3. Edit the generated portrait\n",
    "config_builder.add_column(\n",
    "    dd.ImageColumnConfig(\n",
    "        name=\"edited_portrait\",\n",
    "        prompt=(\n",
    "            \"Edit this {{ animal }} portrait photo. \"\n",
    "            \"Add {{ accessory }} on the animal. \"\n",
    "            \"Place the {{ animal }} in {{ setting }}. \"\n",
    "            \"Render the result in {{ art_style }}. \"\n",
    "            \"Keep the animal's face, expression, and features faithful to the original photo.\"\n",
    "        ),\n",
    "        model_alias=MODEL_ALIAS,\n",
    "        multi_modal_context=[dd.ImageContext(column_name=\"animal_portrait\")],\n",
    "    )\n",
    ")\n",
    "\n",
    "data_designer.validate(config_builder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85516cec",
   "metadata": {},
   "source": [
    "### üîÅ Preview: quick iteration\n",
    "\n",
    "In **preview** mode, generated images are stored as base64 strings in the dataframe. Use this to iterate on your prompts, accessories, and sampler values before scaling up.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e922030",
   "metadata": {},
   "outputs": [],
   "source": [
    "preview = data_designer.preview(config_builder, num_records=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534c5d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(preview.dataset)):\n",
    "    preview.display_sample_record()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff24137",
   "metadata": {},
   "outputs": [],
   "source": [
    "preview.dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9ab4f9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### üîé Compare original vs edited\n",
    "\n",
    "Let's display the generated animal portraits next to their edited versions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a28381",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image(image_value, base_path: Path | None = None) -> None:\n",
    "    \"\"\"Display an image from base64 (preview mode) or file path (create mode).\"\"\"\n",
    "    values = image_value if isinstance(image_value, list) else [image_value]\n",
    "    for value in values:\n",
    "        if base_path is not None:\n",
    "            display(IPImage(filename=str(base_path / value)))\n",
    "        else:\n",
    "            display(IPImage(data=base64.b64decode(value)))\n",
    "\n",
    "\n",
    "def display_before_after(row, index: int, base_path: Path | None = None) -> None:\n",
    "    \"\"\"Display original portrait vs edited version for a single record.\"\"\"\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"Record {index}: {row['animal']} wearing {row['accessory']}\")\n",
    "    print(f\"Setting: {row['setting']}, Style: {row['art_style']}\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "\n",
    "    print(\"\\nüì∑ Generated portrait:\")\n",
    "    display_image(row[\"animal_portrait\"], base_path)\n",
    "\n",
    "    print(\"\\nüé® Edited version:\")\n",
    "    display_image(row[\"edited_portrait\"], base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd69086",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in preview.dataset.iterrows():\n",
    "    display_before_after(row, index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5089b88",
   "metadata": {},
   "source": [
    "### üÜô Create at scale\n",
    "\n",
    "In **create** mode, images are saved to disk in `images/<column_name>/` folders with UUID filenames. The dataframe stores relative paths. `ImageContext` auto-detection handles this transparently‚Äîgenerated file paths are resolved to base64 before being sent to the model for editing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd91d3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = data_designer.create(config_builder, num_records=5, dataset_name=\"tutorial-6-edited-images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecf8f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = results.load_dataset()\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895767fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in dataset.head(10).iterrows():\n",
    "    display_before_after(row, index, base_path=results.artifact_storage.base_dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe53fc8",
   "metadata": {},
   "source": [
    "## ‚è≠Ô∏è Next steps\n",
    "\n",
    "- Experiment with different autoregressive models for image generation and editing\n",
    "- Try more creative editing prompts (style transfer, background replacement, artistic filters)\n",
    "- Combine image generation with text generation (e.g., generate captions using an LLM-Text column with `ImageContext`)\n",
    "- Chain more than two image columns for multi-step editing pipelines\n",
    "\n",
    "Related tutorials:\n",
    "\n",
    "- [The basics](https://nvidia-nemo.github.io/DataDesigner/latest/notebooks/1-the-basics/): samplers and LLM text columns\n",
    "- [Providing images as context](https://nvidia-nemo.github.io/DataDesigner/latest/notebooks/4-providing-images-as-context/): image-to-text with VLMs\n",
    "- [Generating images](https://nvidia-nemo.github.io/DataDesigner/latest/notebooks/5-generating-images/): text-to-image generation with diffusion models\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
