# SPDX-FileCopyrightText: Copyright (c) 2025-2026 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: Apache-2.0

from __future__ import annotations

from pathlib import Path
from typing import TYPE_CHECKING

from data_designer.config.analysis.dataset_profiler import DatasetProfilerResults
from data_designer.config.config_builder import DataDesignerConfigBuilder
from data_designer.config.dataset_metadata import DatasetMetadata
from data_designer.config.utils.visualization import WithRecordSamplerMixin
from data_designer.engine.dataset_builders.errors import ArtifactStorageError
from data_designer.engine.storage.artifact_storage import ArtifactStorage
from data_designer.integrations.huggingface.client import HuggingFaceHubClient

if TYPE_CHECKING:
    import pandas as pd


class DatasetCreationResults(WithRecordSamplerMixin):
    """Results container for a Data Designer dataset creation run.

    This class provides access to the generated dataset, profiling analysis, and
    visualization utilities. It is returned by the DataDesigner.create() method
    and implements ResultsProtocol of the DataDesigner interface.
    """

    def __init__(
        self,
        *,
        artifact_storage: ArtifactStorage,
        analysis: DatasetProfilerResults,
        config_builder: DataDesignerConfigBuilder,
        dataset_metadata: DatasetMetadata,
    ):
        """Creates a new instance with results based on a dataset creation run.

        Args:
            artifact_storage: Storage manager for accessing generated artifacts.
            analysis: Profiling results for the generated dataset.
            config_builder: Configuration builder used to create the dataset.
            dataset_metadata: Metadata about the generated dataset (e.g., seed column names).
        """
        self.artifact_storage = artifact_storage
        self._analysis = analysis
        self._config_builder = config_builder
        self.dataset_metadata = dataset_metadata

    def load_analysis(self) -> DatasetProfilerResults:
        """Load the profiling analysis results for the generated dataset.

        Returns:
            DatasetProfilerResults containing statistical analysis and quality metrics
                for each column in the generated dataset.
        """
        return self._analysis

    def load_dataset(self) -> pd.DataFrame:
        """Load the generated dataset as a pandas DataFrame.

        Returns:
            A pandas DataFrame containing the full generated dataset.
        """
        return self.artifact_storage.load_dataset()

    def load_processor_dataset(self, processor_name: str) -> pd.DataFrame:
        """Load the dataset generated by a processor.

        This only works for processors that write their artifacts in Parquet format.

        Args:
            processor_name: The name of the processor to load the dataset from.

        Returns:
            A pandas DataFrame containing the dataset generated by the processor.
        """
        try:
            dataset = self.artifact_storage.read_parquet_files(
                self.artifact_storage.processors_outputs_path / processor_name
            )
        except Exception as e:
            raise ArtifactStorageError(f"Failed to load dataset for processor {processor_name}: {e}")

        return dataset

    def get_path_to_processor_artifacts(self, processor_name: str) -> Path:
        """Get the path to the artifacts generated by a processor.

        Args:
            processor_name: The name of the processor to load the artifact from.

        Returns:
            The path to the artifacts.
        """
        if not self.artifact_storage.processors_outputs_path.exists():
            raise ArtifactStorageError(f"Processor {processor_name} has no artifacts.")
        return self.artifact_storage.processors_outputs_path / processor_name

    def push_to_hub(
        self,
        repo_id: str,
        description: str,
        *,
        token: str | None = None,
        private: bool = False,
        tags: list[str] | None = None,
    ) -> str:
        """Push dataset to HuggingFace Hub.

        Uploads all artifacts including:
        - Main parquet batch files (data subset)
        - Processor output batch files ({processor_name} subsets)
        - Configuration (builder_config.json)
        - Metadata (metadata.json)
        - Auto-generated dataset card (README.md)

        Args:
            repo_id: HuggingFace repo ID (e.g., "username/my-dataset")
            description: Custom description text for the dataset card.
                Appears after the title.
            token: HuggingFace API token. If None, the token is automatically
                resolved from HF_TOKEN environment variable or cached credentials
                from `hf auth login`.
            private: Create private repo
            tags: Additional custom tags for the dataset.

        Returns:
            URL to the uploaded dataset

        Example:
            >>> results = data_designer.create(config, num_records=1000)
            >>> description = "This dataset contains synthetic conversations for training chatbots."
            >>> results.push_to_hub("username/my-synthetic-dataset", description, tags=["chatbot", "conversation"])
            'https://huggingface.co/datasets/username/my-synthetic-dataset'
        """
        client = HuggingFaceHubClient(token=token)
        return client.upload_dataset(
            repo_id=repo_id,
            base_dataset_path=self.artifact_storage.base_dataset_path,
            private=private,
            description=description,
            tags=tags,
        )
